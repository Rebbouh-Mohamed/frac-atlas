{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "650de1ff",
   "metadata": {
    "papermill": {
     "duration": 0.004898,
     "end_time": "2026-01-29T06:03:08.019342",
     "exception": false,
     "start_time": "2026-01-29T06:03:08.014444",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# YOLOv8 Bone Fracture Segmentation\n",
    "This notebook performs single-class segmentation training on bone fracture data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ef6162a",
   "metadata": {
    "papermill": {
     "duration": 0.00368,
     "end_time": "2026-01-29T06:03:08.027362",
     "exception": false,
     "start_time": "2026-01-29T06:03:08.023682",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Setup and Configuration\n",
    "Declare all configuration variables and install dependencies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02f943b6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T06:03:08.035972Z",
     "iopub.status.busy": "2026-01-29T06:03:08.035696Z",
     "iopub.status.idle": "2026-01-29T06:03:08.158172Z",
     "shell.execute_reply": "2026-01-29T06:03:08.157327Z"
    },
    "papermill": {
     "duration": 0.129127,
     "end_time": "2026-01-29T06:03:08.160070",
     "exception": false,
     "start_time": "2026-01-29T06:03:08.030943",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory â€˜./resultsâ€™: File exists\n",
      "Collecting pandas\n",
      "  Downloading pandas-3.0.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (79 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m467.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m kB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting Pillow\n",
      "  Using cached pillow-12.1.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
      "Collecting numpy>=1.26.0 (from pandas)\n",
      "  Downloading numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-3.0.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (10.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m879.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pillow-12.1.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
      "Downloading numpy-2.4.2-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.6 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m16.6/16.6 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: Pillow, numpy, pandas\n",
      "Successfully installed Pillow-12.1.0 numpy-2.4.2 pandas-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!mkdir ./results \n",
    "!pip install pandas Pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89d7e2fa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T06:03:08.169999Z",
     "iopub.status.busy": "2026-01-29T06:03:08.169700Z",
     "iopub.status.idle": "2026-01-29T06:03:16.287197Z",
     "shell.execute_reply": "2026-01-29T06:03:16.286162Z"
    },
    "papermill": {
     "duration": 8.125116,
     "end_time": "2026-01-29T06:03:16.288877",
     "exception": false,
     "start_time": "2026-01-29T06:03:08.163761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Starting COCO to YOLO Segmentation Conversion\n",
      "============================================================\n",
      "ğŸ“– Loading COCO JSON file...\n",
      "  âœ… Found 1 categories: ['fractured']\n",
      "  âœ… Found 717 images in COCO JSON\n",
      "  âœ… Loaded 922 annotations for 717 images\n",
      "âœ… Directory structure created!\n",
      "\n",
      "ğŸ“‚ Processing train split...\n",
      "  Processed 100/574...\n",
      "  Processed 200/574...\n",
      "  Processed 300/574...\n",
      "  Processed 400/574...\n",
      "  Processed 500/574...\n",
      "  âœ… train: 574/574 successful\n",
      "\n",
      "ğŸ“‚ Processing test split...\n",
      "  âœ… test: 61/61 successful\n",
      "\n",
      "ğŸ“‚ Processing valid split...\n",
      "  âœ… valid: 82/82 successful\n",
      "\n",
      "ğŸ“„ YAML config created: data/dataset.yaml\n",
      "\n",
      "============================================================\n",
      "COCO TO YOLO SEGMENTATION CONVERSION SUMMARY\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š Categories (1): ['fractured']\n",
      "\n",
      "TRAIN : Images:  574 | Labels:  574 | Annotations:  764\n",
      "\n",
      "TEST  : Images:   61 | Labels:   61 | Annotations:   67\n",
      "\n",
      "VALID : Images:   82 | Labels:   82 | Annotations:   91\n",
      "\n",
      "TOTAL : Images:  717 | Labels:  717 | Annotations:  922\n",
      "\n",
      "ğŸ“ Directory Structure:\n",
      "./data/\n",
      "  â”œâ”€â”€ train/\n",
      "  â”‚   â”œâ”€â”€ images/  (574 files)\n",
      "  â”‚   â””â”€â”€ labels/  (574 files)\n",
      "  â”œâ”€â”€ test/\n",
      "  â”‚   â”œâ”€â”€ images/  (61 files)\n",
      "  â”‚   â””â”€â”€ labels/  (61 files)\n",
      "  â”œâ”€â”€ valid/\n",
      "  â”‚   â”œâ”€â”€ images/  (82 files)\n",
      "  â”‚   â””â”€â”€ labels/  (82 files)\n",
      "  â””â”€â”€ dataset.yaml\n",
      "\n",
      "âœ… Conversion complete!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "# Configuration\n",
    "CONFIG = {\n",
    "    'csv_files': {\n",
    "        'train': './FracAtlas/Utilities/Fracture Split/train.csv',\n",
    "        'test': './FracAtlas/Utilities/Fracture Split/test.csv', \n",
    "        'valid': './FracAtlas/Utilities/Fracture Split/valid.csv'\n",
    "    },\n",
    "    'source_paths': {\n",
    "        'images': './FracAtlas/images/Fractured',\n",
    "        'coco_json': './FracAtlas/Annotations/COCO JSON/COCO_fracture_masks.json'\n",
    "    },\n",
    "    'output_path': './data'\n",
    "}\n",
    "\n",
    "class COCOtoYOLOConverter:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.stats = {\n",
    "            'total': {'images': 0, 'labels': 0, 'annotations': 0},\n",
    "            'train': {'images': 0, 'labels': 0, 'annotations': 0},\n",
    "            'test': {'images': 0, 'labels': 0, 'annotations': 0},\n",
    "            'valid': {'images': 0, 'labels': 0, 'annotations': 0}\n",
    "        }\n",
    "        self.coco_data = None\n",
    "        self.image_annotations = {}\n",
    "        self.image_id_to_info = {}\n",
    "        self.category_mapping = {}\n",
    "        \n",
    "    def load_coco_json(self):\n",
    "        \"\"\"Load and parse COCO JSON file\"\"\"\n",
    "        print(\"ğŸ“– Loading COCO JSON file...\")\n",
    "        \n",
    "        with open(self.config['source_paths']['coco_json'], 'r') as f:\n",
    "            self.coco_data = json.load(f)\n",
    "        \n",
    "        # Create category mapping (COCO category_id to YOLO class_id)\n",
    "        categories = self.coco_data.get('categories', [])\n",
    "        self.category_mapping = {cat['id']: idx for idx, cat in enumerate(categories)}\n",
    "        self.category_names = [cat['name'] for cat in categories]\n",
    "        \n",
    "        print(f\"  âœ… Found {len(categories)} categories: {self.category_names}\")\n",
    "        \n",
    "        # Create image_id to image info mapping\n",
    "        images = self.coco_data.get('images', [])\n",
    "        for img in images:\n",
    "            self.image_id_to_info[img['id']] = img\n",
    "        \n",
    "        print(f\"  âœ… Found {len(images)} images in COCO JSON\")\n",
    "        \n",
    "        # Group annotations by image_id\n",
    "        annotations = self.coco_data.get('annotations', [])\n",
    "        for ann in annotations:\n",
    "            image_id = ann['image_id']\n",
    "            if image_id not in self.image_annotations:\n",
    "                self.image_annotations[image_id] = []\n",
    "            self.image_annotations[image_id].append(ann)\n",
    "        \n",
    "        print(f\"  âœ… Loaded {len(annotations)} annotations for {len(self.image_annotations)} images\")\n",
    "    \n",
    "    def convert_segmentation_to_yolo(self, segmentation, img_width, img_height):\n",
    "        \"\"\"\n",
    "        Convert COCO segmentation to YOLO segmentation format\n",
    "        COCO: [[x1, y1, x2, y2, ...]] or [x1, y1, x2, y2, ...]\n",
    "        YOLO: [x1, y1, x2, y2, ...] (normalized 0-1)\n",
    "        \"\"\"\n",
    "        if not segmentation or len(segmentation) == 0:\n",
    "            return None\n",
    "        \n",
    "        # Handle both formats: list of lists or single list\n",
    "        if isinstance(segmentation, list):\n",
    "            # If it's a list of polygons, take the first one (or you can merge them)\n",
    "            polygon = segmentation[0] if isinstance(segmentation[0], list) else segmentation\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "        # Ensure we have pairs of coordinates\n",
    "        if len(polygon) < 6:  # At least 3 points (6 values)\n",
    "            return None\n",
    "        \n",
    "        # Normalize coordinates\n",
    "        normalized_coords = []\n",
    "        for i in range(0, len(polygon), 2):\n",
    "            if i + 1 < len(polygon):\n",
    "                x = polygon[i] / img_width\n",
    "                y = polygon[i + 1] / img_height\n",
    "                # Clamp values between 0 and 1\n",
    "                x = max(0.0, min(1.0, x))\n",
    "                y = max(0.0, min(1.0, y))\n",
    "                normalized_coords.extend([x, y])\n",
    "        \n",
    "        return normalized_coords\n",
    "    \n",
    "    def create_yolo_annotation(self, image_id, img_width, img_height):\n",
    "        \"\"\"Create YOLO format annotation for a single image using SEGMENTATION only\"\"\"\n",
    "        if image_id not in self.image_annotations:\n",
    "            return []\n",
    "        \n",
    "        yolo_lines = []\n",
    "        \n",
    "        for ann in self.image_annotations[image_id]:\n",
    "            category_id = ann['category_id']\n",
    "            class_id = self.category_mapping.get(category_id, 0)\n",
    "            \n",
    "            # Use SEGMENTATION only (not bbox)\n",
    "            if 'segmentation' in ann and ann['segmentation']:\n",
    "                coords = self.convert_segmentation_to_yolo(\n",
    "                    ann['segmentation'], img_width, img_height\n",
    "                )\n",
    "                \n",
    "                if coords and len(coords) >= 6:  # At least 3 points\n",
    "                    # YOLO segmentation format: class_id x1 y1 x2 y2 x3 y3 ...\n",
    "                    line = f\"{class_id} \" + \" \".join(f\"{coord:.6f}\" for coord in coords)\n",
    "                    yolo_lines.append(line)\n",
    "        \n",
    "        return yolo_lines\n",
    "    \n",
    "    def setup_directories(self):\n",
    "        \"\"\"Create YOLO directory structure\"\"\"\n",
    "        for split in ['train', 'test', 'valid']:\n",
    "            Path(self.config['output_path']).mkdir(parents=True, exist_ok=True)\n",
    "            Path(f\"{self.config['output_path']}/{split}/images\").mkdir(parents=True, exist_ok=True)\n",
    "            Path(f\"{self.config['output_path']}/{split}/labels\").mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        print(\"âœ… Directory structure created!\")\n",
    "    \n",
    "    def find_image_by_filename(self, filename):\n",
    "        \"\"\"Find COCO image info by filename\"\"\"\n",
    "        # Try exact match first\n",
    "        for img_id, img_info in self.image_id_to_info.items():\n",
    "            if img_info.get('file_name') == filename:\n",
    "                return img_id, img_info\n",
    "        \n",
    "        # Try with .jpg extension\n",
    "        filename_with_jpg = filename if filename.endswith('.jpg') else f\"{filename}.jpg\"\n",
    "        for img_id, img_info in self.image_id_to_info.items():\n",
    "            if img_info.get('file_name') == filename_with_jpg:\n",
    "                return img_id, img_info\n",
    "        \n",
    "        # Try without extension\n",
    "        filename_no_ext = filename.replace('.jpg', '')\n",
    "        for img_id, img_info in self.image_id_to_info.items():\n",
    "            file_name = img_info.get('file_name', '')\n",
    "            if file_name.replace('.jpg', '') == filename_no_ext:\n",
    "                return img_id, img_info\n",
    "        \n",
    "        return None, None\n",
    "    \n",
    "    def process_image(self, csv_image_id, split):\n",
    "        \"\"\"Process a single image: copy image and create YOLO label with segmentation\"\"\"\n",
    "        # Clean image_id\n",
    "        image_id_clean = csv_image_id.split(\".\")[0]\n",
    "        image_file = f\"{image_id_clean}.jpg\"\n",
    "        label_file = f\"{image_id_clean}.txt\"\n",
    "        \n",
    "        # Source paths\n",
    "        src_image = Path(self.config['source_paths']['images']) / image_file\n",
    "        \n",
    "        # Destination paths\n",
    "        dest_image = Path(f\"{self.config['output_path']}/{split}/images\") / image_file\n",
    "        dest_label = Path(f\"{self.config['output_path']}/{split}/labels\") / label_file\n",
    "        \n",
    "        # Check if image exists\n",
    "        if not src_image.exists():\n",
    "            print(f\"  âš ï¸  Image missing: {image_file}\")\n",
    "            return False\n",
    "        \n",
    "        # Find corresponding COCO image_id and info\n",
    "        coco_image_id, img_info = self.find_image_by_filename(image_file)\n",
    "        \n",
    "        if coco_image_id is None:\n",
    "            print(f\"  âš ï¸  Image not found in COCO JSON: {image_file}\")\n",
    "            return False\n",
    "        \n",
    "        # Get image dimensions from COCO data\n",
    "        img_width = img_info.get('width')\n",
    "        img_height = img_info.get('height')\n",
    "        \n",
    "        # If dimensions not in COCO, read from actual image\n",
    "        if img_width is None or img_height is None:\n",
    "            try:\n",
    "                with Image.open(src_image) as img:\n",
    "                    img_width, img_height = img.size\n",
    "            except Exception as e:\n",
    "                print(f\"  âš ï¸  Could not read image dimensions for {image_file}: {e}\")\n",
    "                return False\n",
    "        \n",
    "        # Create YOLO annotation from segmentation\n",
    "        yolo_lines = self.create_yolo_annotation(coco_image_id, img_width, img_height)\n",
    "        \n",
    "        if not yolo_lines:\n",
    "            print(f\"  âš ï¸  No segmentation annotations found for: {image_file}\")\n",
    "            # Still copy image but create empty label file\n",
    "        \n",
    "        # Copy image\n",
    "        try:\n",
    "            shutil.copy2(src_image, dest_image)\n",
    "            self.stats[split]['images'] += 1\n",
    "            self.stats['total']['images'] += 1\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ  Error copying image {image_file}: {e}\")\n",
    "            return False\n",
    "        \n",
    "        # Write YOLO label file\n",
    "        try:\n",
    "            with open(dest_label, 'w') as f:\n",
    "                f.write('\\n'.join(yolo_lines))\n",
    "            \n",
    "            self.stats[split]['labels'] += 1\n",
    "            self.stats['total']['labels'] += 1\n",
    "            self.stats[split]['annotations'] += len(yolo_lines)\n",
    "            self.stats['total']['annotations'] += len(yolo_lines)\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ  Error writing label {label_file}: {e}\")\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def process_split(self, split_name):\n",
    "        \"\"\"Process a single split (train/test/valid)\"\"\"\n",
    "        csv_path = self.config['csv_files'][split_name]\n",
    "        \n",
    "        if not Path(csv_path).exists():\n",
    "            print(f\"âŒ CSV file not found: {csv_path}\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\nğŸ“‚ Processing {split_name} split...\")\n",
    "        \n",
    "        # Read CSV\n",
    "        df = pd.read_csv(csv_path)\n",
    "        total_items = len(df)\n",
    "        successful = 0\n",
    "        \n",
    "        # Process each image\n",
    "        for idx, row in df.iterrows():\n",
    "            image_id = row['image_id']\n",
    "            if self.process_image(image_id, split_name):\n",
    "                successful += 1\n",
    "            \n",
    "            # Progress indicator\n",
    "            if (idx + 1) % 100 == 0:\n",
    "                print(f\"  Processed {idx + 1}/{total_items}...\")\n",
    "        \n",
    "        print(f\"  âœ… {split_name}: {successful}/{total_items} successful\")\n",
    "    \n",
    "    def create_yaml_config(self):\n",
    "        \"\"\"Create dataset.yaml for YOLO training\"\"\"\n",
    "        yaml_content = f\"\"\"# YOLO Dataset Configuration\n",
    "path: {self.config['output_path']}\n",
    "train: train/images\n",
    "val: valid/images\n",
    "test: test/images\n",
    "\n",
    "# Classes\n",
    "nc: {len(self.category_names)}  # number of classes\n",
    "names: {self.category_names}\n",
    "\n",
    "# Dataset info\n",
    "# Converted from COCO segmentation format\n",
    "# Total images: {self.stats['total']['images']}\n",
    "# Total labels: {self.stats['total']['labels']}\n",
    "# Total segmentation annotations: {self.stats['total']['annotations']}\n",
    "\"\"\"\n",
    "        \n",
    "        yaml_path = Path(self.config['output_path']) / 'dataset.yaml'\n",
    "        with open(yaml_path, 'w') as f:\n",
    "            f.write(yaml_content)\n",
    "        \n",
    "        print(f\"\\nğŸ“„ YAML config created: {yaml_path}\")\n",
    "        return yaml_path\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Print summary statistics\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"COCO TO YOLO SEGMENTATION CONVERSION SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        print(f\"\\nğŸ“Š Categories ({len(self.category_names)}): {self.category_names}\")\n",
    "        \n",
    "        for split in ['train', 'test', 'valid']:\n",
    "            print(f\"\\n{split.upper():6s}: \"\n",
    "                  f\"Images: {self.stats[split]['images']:4d} | \"\n",
    "                  f\"Labels: {self.stats[split]['labels']:4d} | \"\n",
    "                  f\"Annotations: {self.stats[split]['annotations']:4d}\")\n",
    "        \n",
    "        print(f\"\\n{'TOTAL':6s}: \"\n",
    "              f\"Images: {self.stats['total']['images']:4d} | \"\n",
    "              f\"Labels: {self.stats['total']['labels']:4d} | \"\n",
    "              f\"Annotations: {self.stats['total']['annotations']:4d}\")\n",
    "        \n",
    "        # Print directory tree\n",
    "        print(f\"\\nğŸ“ Directory Structure:\")\n",
    "        print(f\"{self.config['output_path']}/\")\n",
    "        for split in ['train', 'test', 'valid']:\n",
    "            print(f\"  â”œâ”€â”€ {split}/\")\n",
    "            print(f\"  â”‚   â”œâ”€â”€ images/  ({self.stats[split]['images']} files)\")\n",
    "            print(f\"  â”‚   â””â”€â”€ labels/  ({self.stats[split]['labels']} files)\")\n",
    "        print(f\"  â””â”€â”€ dataset.yaml\")\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Main execution method\"\"\"\n",
    "        print(\"ğŸš€ Starting COCO to YOLO Segmentation Conversion\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Load COCO JSON\n",
    "        self.load_coco_json()\n",
    "        \n",
    "        # Setup directories\n",
    "        self.setup_directories()\n",
    "        \n",
    "        # Process all splits\n",
    "        for split in ['train', 'test', 'valid']:\n",
    "            self.process_split(split)\n",
    "        \n",
    "        # Create YAML config\n",
    "        self.create_yaml_config()\n",
    "        \n",
    "        # Print summary\n",
    "        self.print_summary()\n",
    "        \n",
    "        print(\"\\nâœ… Conversion complete!\")\n",
    "\n",
    "\n",
    "# Run the converter\n",
    "if __name__ == \"__main__\":\n",
    "    converter = COCOtoYOLOConverter(CONFIG)\n",
    "    converter.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21b70077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (11 kB)\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy>=1.24.1 in ./venv/lib/python3.12/site-packages (from scikit-learn) (2.4.2)\n",
      "Requirement already satisfied: scipy>=1.10.0 in ./venv/lib/python3.12/site-packages (from scikit-learn) (1.17.0)\n",
      "Collecting joblib>=1.3.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.3-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting threadpoolctl>=3.2.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: pandas>=1.2 in ./venv/lib/python3.12/site-packages (from seaborn) (3.0.0)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in ./venv/lib/python3.12/site-packages (from seaborn) (3.10.8)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./venv/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in ./venv/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./venv/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./venv/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\n",
      "Requirement already satisfied: packaging>=20.0 in ./venv/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (26.0)\n",
      "Requirement already satisfied: pillow>=8 in ./venv/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (12.1.0)\n",
      "Requirement already satisfied: pyparsing>=3 in ./venv/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./venv/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./venv/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Downloading scikit_learn-1.8.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (8.9 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0mm\n",
      "\u001b[?25hDownloading seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading joblib-1.5.3-py3-none-any.whl (309 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m309.1/309.1 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn, seaborn\n",
      "Successfully installed joblib-1.5.3 scikit-learn-1.8.0 seaborn-0.13.2 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42204f71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T06:03:16.298196Z",
     "iopub.status.busy": "2026-01-29T06:03:16.297929Z",
     "iopub.status.idle": "2026-01-29T06:03:35.964799Z",
     "shell.execute_reply": "2026-01-29T06:03:35.963736Z"
    },
    "papermill": {
     "duration": 19.673923,
     "end_time": "2026-01-29T06:03:35.966977",
     "exception": false,
     "start_time": "2026-01-29T06:03:16.293054",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install ultralytics albumentations tqdm opencv-python-headless\n",
    "import os\n",
    "import cv2\n",
    "import yaml\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "import albumentations as A\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "\n",
    "# --- 1. Configuration Variables ---\n",
    "INPUT_DIR = './'\n",
    "WORKING_DIR = './'\n",
    "DATASET_DIR = os.path.join(WORKING_DIR, 'data')\n",
    "RESULTS_DIR = os.path.join(WORKING_DIR, 'results')\n",
    "\n",
    "# Paths for train/val/test\n",
    "FOLDERS = ['train', 'valid', 'test']\n",
    "\n",
    "# Model Parameters\n",
    "MODEL_NAME = 'yolo11m-seg.pt'\n",
    "IMG_SIZE = 800\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 100\n",
    "LR = 0.001\n",
    "\n",
    "# Augmentation Parameters (Offline)\n",
    "AUG_COUNT = 10 # Number of augmented versions per training image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9badfc97",
   "metadata": {
    "papermill": {
     "duration": 0.005148,
     "end_time": "2026-01-29T06:03:36.007206",
     "exception": false,
     "start_time": "2026-01-29T06:03:36.002058",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 3. Data Augmentation (Training Set Only)\n",
    "Apply offline augmentation to enhance the model performance on medical images.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f0672e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T06:03:36.038188Z",
     "iopub.status.busy": "2026-01-29T06:03:36.037695Z",
     "iopub.status.idle": "2026-01-29T06:03:59.851698Z",
     "shell.execute_reply": "2026-01-29T06:03:59.850911Z"
    },
    "papermill": {
     "duration": 23.822566,
     "end_time": "2026-01-29T06:03:59.853142",
     "exception": false,
     "start_time": "2026-01-29T06:03:36.030576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "import cv2\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def get_augmentations():\n",
    "    \"\"\"Returns a list of augmentation pipelines for ~5x data augmentation\"\"\"\n",
    "    augmentation_pipelines = [\n",
    "        # Pipeline 1: Horizontal Flip\n",
    "        A.Compose([\n",
    "            A.HorizontalFlip(p=1.0),\n",
    "        ], keypoint_params=A.KeypointParams(format='xy', remove_invisible=False)),\n",
    "        \n",
    "        # Pipeline 2: Vertical Flip\n",
    "        A.Compose([\n",
    "            A.VerticalFlip(p=1.0),\n",
    "        ], keypoint_params=A.KeypointParams(format='xy', remove_invisible=False)),\n",
    "        \n",
    "        # Pipeline 3: Rotation + Brightness\n",
    "        A.Compose([\n",
    "            A.Rotate(limit=15, p=1.0, border_mode=cv2.BORDER_CONSTANT),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.8),\n",
    "        ], keypoint_params=A.KeypointParams(format='xy', remove_invisible=False)),\n",
    "        \n",
    "        # Pipeline 4: Affine + Contrast\n",
    "        A.Compose([\n",
    "            A.Affine(scale=(0.85, 1.15), translate_percent=0.1, rotate=(-10, 10),\n",
    "                     p=1.0, mode=cv2.BORDER_CONSTANT),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.15, contrast_limit=0.15, p=0.5),\n",
    "        ], keypoint_params=A.KeypointParams(format='xy', remove_invisible=False)),\n",
    "        \n",
    "        # Pipeline 5: Noise + Blur\n",
    "        A.Compose([\n",
    "            A.OneOf([\n",
    "                A.GaussNoise(var_limit=(10.0, 50.0), p=1.0),\n",
    "                A.GaussianBlur(blur_limit=(3, 5), p=1.0),\n",
    "                A.MedianBlur(blur_limit=3, p=1.0),\n",
    "            ], p=1.0),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.2, p=0.7),\n",
    "        ], keypoint_params=A.KeypointParams(format='xy', remove_invisible=False)),\n",
    "    ]\n",
    "    \n",
    "    return augmentation_pipelines\n",
    "\n",
    "\n",
    "def parse_yolo_segmentation(label_line):\n",
    "    \"\"\"\n",
    "    Parse YOLO segmentation format line\n",
    "    Returns: class_id, list of normalized polygon coords\n",
    "    \"\"\"\n",
    "    parts = label_line.strip().split()\n",
    "    if len(parts) < 7:  # class + at least 3 points (6 coords)\n",
    "        return None, None\n",
    "    \n",
    "    class_id = int(parts[0])\n",
    "    coords = [float(x) for x in parts[1:]]\n",
    "    \n",
    "    # Ensure we have pairs of coordinates\n",
    "    if len(coords) % 2 != 0:\n",
    "        coords = coords[:-1]  # Remove last odd coordinate\n",
    "    \n",
    "    return class_id, coords\n",
    "\n",
    "\n",
    "def polygon_to_keypoints(polygon_coords, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Convert normalized YOLO polygon coords to absolute pixel keypoints\n",
    "    Input: [x1_norm, y1_norm, x2_norm, y2_norm, ...]\n",
    "    Output: [(x1_abs, y1_abs), (x2_abs, y2_abs), ...]\n",
    "    \"\"\"\n",
    "    keypoints = []\n",
    "    for i in range(0, len(polygon_coords), 2):\n",
    "        x_abs = polygon_coords[i] * img_width\n",
    "        y_abs = polygon_coords[i+1] * img_height\n",
    "        keypoints.append((x_abs, y_abs))\n",
    "    return keypoints\n",
    "\n",
    "\n",
    "def keypoints_to_polygon(keypoints, img_width, img_height):\n",
    "    \"\"\"\n",
    "    Convert absolute pixel keypoints back to normalized YOLO polygon coords\n",
    "    Input: [(x1_abs, y1_abs), (x2_abs, y2_abs), ...]\n",
    "    Output: [x1_norm, y1_norm, x2_norm, y2_norm, ...]\n",
    "    \"\"\"\n",
    "    polygon = []\n",
    "    for x, y in keypoints:\n",
    "        # Normalize and clamp to [0, 1]\n",
    "        x_norm = max(0.0, min(1.0, x / img_width))\n",
    "        y_norm = max(0.0, min(1.0, y / img_height))\n",
    "        polygon.extend([x_norm, y_norm])\n",
    "    return polygon\n",
    "\n",
    "\n",
    "def augment_dataset(split='train', num_aug=5):\n",
    "    \"\"\"\n",
    "    Augment dataset with multiple augmentation pipelines\n",
    "    Args:\n",
    "        split: 'train', 'test', or 'valid'\n",
    "        num_aug: number of augmentations per image\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Augmenting {split} set with {num_aug} augmentations per image...\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    DATASET_DIR = '/kaggle/working/data'\n",
    "    img_dir = os.path.join(DATASET_DIR, split, 'images')\n",
    "    lbl_dir = os.path.join(DATASET_DIR, split, 'labels')\n",
    "    \n",
    "    images = glob(os.path.join(img_dir, '*.jpg'))\n",
    "    augmentation_pipelines = get_augmentations()[:num_aug]\n",
    "    \n",
    "    total_augmented = 0\n",
    "    errors = []\n",
    "    skipped = 0\n",
    "    \n",
    "    # Filter out already augmented images\n",
    "    original_images = [img for img in images if '_aug_' not in os.path.basename(img)]\n",
    "    print(f\"Found {len(original_images)} original images to augment\")\n",
    "    \n",
    "    for img_path in tqdm(original_images, desc=f\"Processing {split} images\"):\n",
    "        base_name = os.path.basename(img_path).split('.')[0]\n",
    "        lbl_path = os.path.join(lbl_dir, base_name + '.txt')\n",
    "        \n",
    "        # Skip if no label file\n",
    "        if not os.path.exists(lbl_path):\n",
    "            errors.append(f\"Label missing: {base_name}\")\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        # Read image\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            errors.append(f\"Cannot read image: {img_path}\")\n",
    "            skipped += 1\n",
    "            continue\n",
    "            \n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        # Read and parse labels\n",
    "        with open(lbl_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        if not lines:\n",
    "            errors.append(f\"Empty label file: {base_name}\")\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        # Parse all polygons\n",
    "        annotations = []  # List of (class_id, polygon_coords)\n",
    "        for line in lines:\n",
    "            class_id, coords = parse_yolo_segmentation(line)\n",
    "            if class_id is not None and coords is not None:\n",
    "                annotations.append((class_id, coords))\n",
    "        \n",
    "        if not annotations:\n",
    "            errors.append(f\"No valid polygons found: {base_name}\")\n",
    "            skipped += 1\n",
    "            continue\n",
    "        \n",
    "        # Apply each augmentation pipeline\n",
    "        for aug_idx, aug_pipeline in enumerate(augmentation_pipelines):\n",
    "            try:\n",
    "                # Convert all polygons to keypoints\n",
    "                all_keypoints = []\n",
    "                polygon_metadata = []  # Store (class_id, num_points) for each polygon\n",
    "                \n",
    "                for class_id, poly_coords in annotations:\n",
    "                    keypoints = polygon_to_keypoints(poly_coords, w, h)\n",
    "                    all_keypoints.extend(keypoints)\n",
    "                    polygon_metadata.append((class_id, len(keypoints)))\n",
    "                \n",
    "                # Apply augmentation - NO label_fields parameter\n",
    "                transformed = aug_pipeline(\n",
    "                    image=img_rgb,\n",
    "                    keypoints=all_keypoints\n",
    "                )\n",
    "                \n",
    "                aug_img = transformed['image']\n",
    "                aug_keypoints = transformed['keypoints']\n",
    "                aug_h, aug_w = aug_img.shape[:2]\n",
    "                \n",
    "                # Check if we lost keypoints during augmentation\n",
    "                if len(aug_keypoints) != len(all_keypoints):\n",
    "                    errors.append(f\"Keypoint loss in {base_name}_aug_{aug_idx}: \"\n",
    "                                f\"{len(all_keypoints)} -> {len(aug_keypoints)}\")\n",
    "                    continue\n",
    "                \n",
    "                # Reconstruct polygons from augmented keypoints\n",
    "                aug_annotations = []\n",
    "                start_idx = 0\n",
    "                \n",
    "                for class_id, num_points in polygon_metadata:\n",
    "                    polygon_kps = aug_keypoints[start_idx:start_idx + num_points]\n",
    "                    \n",
    "                    # Convert back to normalized coords\n",
    "                    norm_poly = keypoints_to_polygon(polygon_kps, aug_w, aug_h)\n",
    "                    \n",
    "                    # Validate polygon (should have at least 3 points)\n",
    "                    if len(norm_poly) >= 6:\n",
    "                        aug_annotations.append((class_id, norm_poly))\n",
    "                    else:\n",
    "                        errors.append(f\"Invalid polygon in {base_name}_aug_{aug_idx}\")\n",
    "                    \n",
    "                    start_idx += num_points\n",
    "                \n",
    "                # Skip if we lost annotations\n",
    "                if len(aug_annotations) != len(annotations):\n",
    "                    errors.append(f\"Annotation loss in {base_name}_aug_{aug_idx}: \"\n",
    "                                f\"{len(annotations)} -> {len(aug_annotations)}\")\n",
    "                    continue\n",
    "                \n",
    "                # Save augmented image\n",
    "                aug_base = f\"{base_name}_aug_{aug_idx}\"\n",
    "                aug_img_path = os.path.join(img_dir, aug_base + '.jpg')\n",
    "                aug_lbl_path = os.path.join(lbl_dir, aug_base + '.txt')\n",
    "                \n",
    "                # Convert RGB back to BGR for OpenCV\n",
    "                aug_img_bgr = cv2.cvtColor(aug_img, cv2.COLOR_RGB2BGR)\n",
    "                cv2.imwrite(aug_img_path, aug_img_bgr)\n",
    "                \n",
    "                # Write label file in YOLO format\n",
    "                with open(aug_lbl_path, 'w') as f:\n",
    "                    for class_id, poly_coords in aug_annotations:\n",
    "                        # Format: class_id x1 y1 x2 y2 x3 y3 ...\n",
    "                        coords_str = ' '.join(f'{c:.6f}' for c in poly_coords)\n",
    "                        f.write(f\"{class_id} {coords_str}\\n\")\n",
    "                \n",
    "                total_augmented += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                errors.append(f\"Error in {base_name}_aug_{aug_idx}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Augmentation Summary for {split.upper()}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Original images processed: {len(original_images)}\")\n",
    "    print(f\"Skipped (errors): {skipped}\")\n",
    "    print(f\"Augmented images created: {total_augmented}\")\n",
    "    print(f\"Expected augmentations: {(len(original_images) - skipped) * num_aug}\")\n",
    "    print(f\"Total images now: {len(glob(os.path.join(img_dir, '*.jpg')))}\")\n",
    "    print(f\"Total labels now: {len(glob(os.path.join(lbl_dir, '*.txt')))}\")\n",
    "    \n",
    "    if errors:\n",
    "        print(f\"\\nâš ï¸  Errors encountered: {len(errors)}\")\n",
    "        print(\"First 10 errors:\")\n",
    "        for i, error in enumerate(errors[:10]):\n",
    "            print(f\"  {i+1}. {error}\")\n",
    "        if len(errors) > 10:\n",
    "            print(f\"  ... and {len(errors) - 10} more errors\")\n",
    "    \n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return total_augmented\n",
    "\n",
    "\n",
    "def simple_augment_dataset(split='train', num_augmentations=3):\n",
    "    \"\"\"\n",
    "    Simpler and more reliable augmentation using direct OpenCV operations\n",
    "    Creates horizontal flip, vertical flip, and rotation augmentations\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Simple augmentation for {split} set...\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    DATASET_DIR = '/kaggle/working/data'\n",
    "    img_dir = os.path.join(DATASET_DIR, split, 'images')\n",
    "    lbl_dir = os.path.join(DATASET_DIR, split, 'labels')\n",
    "    \n",
    "    images = glob(os.path.join(img_dir, '*.jpg'))\n",
    "    original_images = [img for img in images if '_aug_' not in os.path.basename(img)]\n",
    "    \n",
    "    total_augmented = 0\n",
    "    errors = []\n",
    "    \n",
    "    print(f\"Found {len(original_images)} original images\")\n",
    "    \n",
    "    for img_path in tqdm(original_images, desc=f\"Processing {split}\"):\n",
    "        base_name = os.path.basename(img_path).split('.')[0]\n",
    "        lbl_path = os.path.join(lbl_dir, base_name + '.txt')\n",
    "        \n",
    "        if not os.path.exists(lbl_path):\n",
    "            errors.append(f\"Label missing: {base_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Read image\n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            errors.append(f\"Cannot read image: {img_path}\")\n",
    "            continue\n",
    "        \n",
    "        h, w = img.shape[:2]\n",
    "        \n",
    "        # Read labels\n",
    "        with open(lbl_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "        \n",
    "        if not lines:\n",
    "            errors.append(f\"Empty label: {base_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Parse annotations\n",
    "        annotations = []\n",
    "        for line in lines:\n",
    "            class_id, coords = parse_yolo_segmentation(line)\n",
    "            if class_id is not None and coords is not None:\n",
    "                annotations.append((class_id, coords))\n",
    "        \n",
    "        if not annotations:\n",
    "            errors.append(f\"No valid annotations: {base_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Augmentation 1: Horizontal Flip\n",
    "        if num_augmentations >= 1:\n",
    "            try:\n",
    "                aug_img = cv2.flip(img, 1)\n",
    "                aug_annotations = []\n",
    "                for class_id, poly_coords in annotations:\n",
    "                    flipped_coords = []\n",
    "                    for i in range(0, len(poly_coords), 2):\n",
    "                        x = 1.0 - poly_coords[i]\n",
    "                        y = poly_coords[i + 1]\n",
    "                        flipped_coords.extend([x, y])\n",
    "                    aug_annotations.append((class_id, flipped_coords))\n",
    "                \n",
    "                save_augmentation(img_dir, lbl_dir, base_name, aug_img, \n",
    "                                aug_annotations, 'hflip')\n",
    "                total_augmented += 1\n",
    "            except Exception as e:\n",
    "                errors.append(f\"HFlip error in {base_name}: {str(e)}\")\n",
    "        \n",
    "        # Augmentation 2: Vertical Flip\n",
    "        if num_augmentations >= 2:\n",
    "            try:\n",
    "                aug_img = cv2.flip(img, 0)\n",
    "                aug_annotations = []\n",
    "                for class_id, poly_coords in annotations:\n",
    "                    flipped_coords = []\n",
    "                    for i in range(0, len(poly_coords), 2):\n",
    "                        x = poly_coords[i]\n",
    "                        y = 1.0 - poly_coords[i + 1]\n",
    "                        flipped_coords.extend([x, y])\n",
    "                    aug_annotations.append((class_id, flipped_coords))\n",
    "                \n",
    "                save_augmentation(img_dir, lbl_dir, base_name, aug_img, \n",
    "                                aug_annotations, 'vflip')\n",
    "                total_augmented += 1\n",
    "            except Exception as e:\n",
    "                errors.append(f\"VFlip error in {base_name}: {str(e)}\")\n",
    "        \n",
    "        # Augmentation 3: Rotation 90 degrees\n",
    "        if num_augmentations >= 3:\n",
    "            try:\n",
    "                aug_img = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
    "                aug_annotations = []\n",
    "                for class_id, poly_coords in annotations:\n",
    "                    rotated_coords = []\n",
    "                    for i in range(0, len(poly_coords), 2):\n",
    "                        x = poly_coords[i]\n",
    "                        y = poly_coords[i + 1]\n",
    "                        # 90Â° clockwise: new_x = y, new_y = 1-x\n",
    "                        new_x = y\n",
    "                        new_y = 1.0 - x\n",
    "                        rotated_coords.extend([new_x, new_y])\n",
    "                    aug_annotations.append((class_id, rotated_coords))\n",
    "                \n",
    "                save_augmentation(img_dir, lbl_dir, base_name, aug_img, \n",
    "                                aug_annotations, 'rot90')\n",
    "                total_augmented += 1\n",
    "            except Exception as e:\n",
    "                errors.append(f\"Rot90 error in {base_name}: {str(e)}\")\n",
    "        \n",
    "        # Augmentation 4: Both flips\n",
    "        if num_augmentations >= 4:\n",
    "            try:\n",
    "                aug_img = cv2.flip(img, -1)  # Both flips\n",
    "                aug_annotations = []\n",
    "                for class_id, poly_coords in annotations:\n",
    "                    flipped_coords = []\n",
    "                    for i in range(0, len(poly_coords), 2):\n",
    "                        x = 1.0 - poly_coords[i]\n",
    "                        y = 1.0 - poly_coords[i + 1]\n",
    "                        flipped_coords.extend([x, y])\n",
    "                    aug_annotations.append((class_id, flipped_coords))\n",
    "                \n",
    "                save_augmentation(img_dir, lbl_dir, base_name, aug_img, \n",
    "                                aug_annotations, 'bothflip')\n",
    "                total_augmented += 1\n",
    "            except Exception as e:\n",
    "                errors.append(f\"BothFlip error in {base_name}: {str(e)}\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Simple Augmentation Summary\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Original images: {len(original_images)}\")\n",
    "    print(f\"Augmented images created: {total_augmented}\")\n",
    "    print(f\"Expected: {len(original_images) * num_augmentations}\")\n",
    "    print(f\"Total images now: {len(glob(os.path.join(img_dir, '*.jpg')))}\")\n",
    "    \n",
    "    if errors:\n",
    "        print(f\"\\nâš ï¸  Errors: {len(errors)}\")\n",
    "        for i, error in enumerate(errors[:5]):\n",
    "            print(f\"  {i+1}. {error}\")\n",
    "    \n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    return total_augmented\n",
    "\n",
    "\n",
    "def save_augmentation(img_dir, lbl_dir, base_name, aug_img, aug_annotations, suffix):\n",
    "    \"\"\"Helper function to save augmented image and labels\"\"\"\n",
    "    aug_base = f\"{base_name}_aug_{suffix}\"\n",
    "    aug_img_path = os.path.join(img_dir, aug_base + '.jpg')\n",
    "    aug_lbl_path = os.path.join(lbl_dir, aug_base + '.txt')\n",
    "    \n",
    "    cv2.imwrite(aug_img_path, aug_img)\n",
    "    \n",
    "    with open(aug_lbl_path, 'w') as f:\n",
    "        for class_id, coords in aug_annotations:\n",
    "            coords_str = ' '.join(f'{c:.6f}' for c in coords)\n",
    "            f.write(f\"{class_id} {coords_str}\\n\")\n",
    "\n",
    "\n",
    "\n",
    "simple_augment_dataset(split='train', num_augmentations=AUG_COUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7db771",
   "metadata": {
    "papermill": {
     "duration": 0.008752,
     "end_time": "2026-01-29T06:03:59.870705",
     "exception": false,
     "start_time": "2026-01-29T06:03:59.861953",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 4. Training YOLOv8\n",
    "Create `data.yaml` and run the training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55e2647",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T06:03:59.888350Z",
     "iopub.status.busy": "2026-01-29T06:03:59.888017Z",
     "iopub.status.idle": "2026-01-29T06:04:00.118124Z",
     "shell.execute_reply": "2026-01-29T06:04:00.117264Z"
    },
    "papermill": {
     "duration": 0.241147,
     "end_time": "2026-01-29T06:04:00.119618",
     "exception": false,
     "start_time": "2026-01-29T06:03:59.878471",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "\n",
    "def validate_yolo_annotations(dataset_dir='/kaggle/working/data'):\n",
    "    \"\"\"Validate all YOLO annotation files\"\"\"\n",
    "    \n",
    "    splits = ['train', 'valid', 'test'] if os.path.exists(os.path.join(dataset_dir, 'val')) else ['train', 'test']\n",
    "    \n",
    "    for split in splits:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Validating {split} annotations...\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        labels_dir = os.path.join(dataset_dir, split, 'labels')\n",
    "        images_dir = os.path.join(dataset_dir, split, 'images')\n",
    "        \n",
    "        label_files = list(Path(labels_dir).glob('*.txt'))\n",
    "        print(f\"Found {len(label_files)} label files\")\n",
    "        \n",
    "        issues = []\n",
    "        valid_count = 0\n",
    "        \n",
    "        for label_file in label_files[:20]:  # Check first 20 files\n",
    "            try:\n",
    "                image_file = label_file.with_suffix('.jpg')\n",
    "                image_file = os.path.join(images_dir, image_file.name)\n",
    "                \n",
    "                if not os.path.exists(image_file):\n",
    "                    issues.append(f\"Image missing: {label_file.name}\")\n",
    "                    continue\n",
    "                \n",
    "                # Read image to get dimensions\n",
    "                img = cv2.imread(image_file)\n",
    "                if img is None:\n",
    "                    issues.append(f\"Cannot read image: {label_file.name}\")\n",
    "                    continue\n",
    "                \n",
    "                h, w = img.shape[:2]\n",
    "                \n",
    "                # Read label file\n",
    "                with open(label_file, 'r') as f:\n",
    "                    lines = f.readlines()\n",
    "                \n",
    "                if not lines:\n",
    "                    issues.append(f\"Empty label file: {label_file.name}\")\n",
    "                    continue\n",
    "                \n",
    "                for line_num, line in enumerate(lines):\n",
    "                    parts = line.strip().split()\n",
    "                    \n",
    "                    # Check minimum length\n",
    "                    if len(parts) < 3:\n",
    "                        issues.append(f\"{label_file.name}: Line {line_num} has only {len(parts)} values (needs at least 3)\")\n",
    "                        continue\n",
    "                    \n",
    "                    try:\n",
    "                        # Parse values\n",
    "                        class_id = int(parts[0])\n",
    "                        polygon = list(map(float, parts[1:]))\n",
    "                        \n",
    "                        # Check polygon length (must be even)\n",
    "                        if len(polygon) % 2 != 0:\n",
    "                            issues.append(f\"{label_file.name}: Line {line_num} has odd number of polygon coordinates: {len(polygon)}\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Check number of points (minimum 3 points for polygon = 6 values)\n",
    "                        if len(polygon) < 6:\n",
    "                            issues.append(f\"{label_file.name}: Line {line_num} has only {len(polygon)//2} points (needs at least 3)\")\n",
    "                            continue\n",
    "                        \n",
    "                        # Check coordinate ranges (should be 0-1)\n",
    "                        coords_array = np.array(polygon)\n",
    "                        if np.any(coords_array < 0) or np.any(coords_array > 1):\n",
    "                            # Try to normalize if coordinates are in pixel values\n",
    "                            if np.any(coords_array > 1):\n",
    "                                issues.append(f\"{label_file.name}: Line {line_num} has coordinates > 1. Max: {coords_array.max()}\")\n",
    "                        \n",
    "                        # Check for duplicate points\n",
    "                        points = [(polygon[i], polygon[i+1]) for i in range(0, len(polygon), 2)]\n",
    "                        if len(points) != len(set(points)):\n",
    "                            issues.append(f\"{label_file.name}: Line {line_num} has duplicate points\")\n",
    "                            \n",
    "                    except ValueError as e:\n",
    "                        issues.append(f\"{label_file.name}: Line {line_num} parsing error: {e}\")\n",
    "                \n",
    "                valid_count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                issues.append(f\"Error processing {label_file.name}: {e}\")\n",
    "        \n",
    "        print(f\"\\nValid files: {valid_count}\")\n",
    "        print(f\"Issues found: {len(issues)}\")\n",
    "        \n",
    "        if issues:\n",
    "            print(\"\\nFirst 10 issues:\")\n",
    "            for issue in issues[:10]:\n",
    "                print(f\"  - {issue}\")\n",
    "            if len(issues) > 10:\n",
    "                print(f\"  ... and {len(issues) - 10} more issues\")\n",
    "        \n",
    "        # Show sample of valid annotations\n",
    "        print(f\"\\nSample annotation format:\")\n",
    "        if label_files:\n",
    "            sample_file = label_files[0]\n",
    "            with open(sample_file, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            if lines:\n",
    "                print(f\"  File: {sample_file.name}\")\n",
    "                print(f\"  First line: {lines[0].strip()}\")\n",
    "                parts = lines[0].strip().split()\n",
    "                print(f\"  - Class ID: {parts[0]}\")\n",
    "                print(f\"  - Number of coordinates: {len(parts[1:])}\")\n",
    "                print(f\"  - Number of polygon points: {len(parts[1:]) // 2}\")\n",
    "\n",
    "# Run validation\n",
    "validate_yolo_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9330d5b4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T06:04:00.138687Z",
     "iopub.status.busy": "2026-01-29T06:04:00.137948Z",
     "iopub.status.idle": "2026-01-29T13:43:06.192069Z",
     "shell.execute_reply": "2026-01-29T13:43:06.190985Z"
    },
    "papermill": {
     "duration": 27546.065841,
     "end_time": "2026-01-29T13:43:06.194063",
     "exception": false,
     "start_time": "2026-01-29T06:04:00.128222",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create data.yaml\n",
    "data_yaml = {\n",
    "    'path': DATASET_DIR,\n",
    "    'train': 'train/images',\n",
    "    'val': 'valid/images',\n",
    "    'test': 'test/images',\n",
    "    'nc': 1,\n",
    "    'names': ['fracture']\n",
    "}\n",
    "\n",
    "with open(os.path.join(WORKING_DIR, 'data.yaml'), 'w') as f:\n",
    "    yaml.dump(data_yaml, f)\n",
    "\n",
    "# Load model\n",
    "model = YOLO(\"./models/v7_best_mine.pt\")\n",
    "\n",
    "Train\n",
    "results = model.train(\n",
    "    data=os.path.join(WORKING_DIR, 'data.yaml'),\n",
    "    epochs=EPOCHS,\n",
    "    imgsz=IMG_SIZE,\n",
    "    batch=BATCH_SIZE,\n",
    "    single_cls=True,\n",
    "    project=WORKING_DIR,\n",
    "    name='bone_fracture_model',\n",
    "    save=True,\n",
    "    plots=True,\n",
    "    lr0=0.001,\n",
    "    max_det=7,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a328241",
   "metadata": {
    "papermill": {
     "duration": 0.726963,
     "end_time": "2026-01-29T13:43:07.744362",
     "exception": false,
     "start_time": "2026-01-29T13:43:07.017399",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 5. Evaluation & Metrics Calculation\\nCalculate ~20 evaluation metrics and generate all required plots.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "842796a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T13:43:09.329672Z",
     "iopub.status.busy": "2026-01-29T13:43:09.328930Z",
     "iopub.status.idle": "2026-01-29T13:43:17.541450Z",
     "shell.execute_reply": "2026-01-29T13:43:17.540358Z"
    },
    "papermill": {
     "duration": 8.970935,
     "end_time": "2026-01-29T13:43:17.543003",
     "exception": false,
     "start_time": "2026-01-29T13:43:08.572068",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last checkpoint\n",
      "Gathering comprehensive metrics...\n",
      "Ultralytics 8.4.11 ğŸš€ Python-3.12.3 torch-2.10.0+cu128 CPU (Intel Core(TM) i5-6300U 2.40GHz)\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 665.1Â±182.7 MB/s, size: 18.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/rebbouh/data_segm/data/valid/labels.cache... 82 images, 0 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 82/82 21.5Mit/s 0.0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95)     Mask(P          R      mAP50  mAP50-95): 100% â”â”â”â”â”â”â”â”â”â”â”â” 6/6 30.9s/it 3:0542.9s07\n",
      "                   all         82         91      0.728       0.56      0.646      0.314      0.699      0.538      0.607      0.279\n",
      "Speed: 6.8ms preprocess, 2238.3ms inference, 0.0ms loss, 3.8ms postprocess per image\n",
      "Results saved to \u001b[1m/home/rebbouh/data_segm/runs/segment/val6\u001b[0m\n",
      "\n",
      "--- Final Metrics Summary ---\n",
      "Precision(Box)      : 0.7279\n",
      "Recall(Box)         : 0.5604\n",
      "mAP50(Box)          : 0.6460\n",
      "mAP50-95(Box)       : 0.3135\n",
      "Precision(Mask)     : 0.6988\n",
      "Recall(Mask)        : 0.5385\n",
      "mAP50(Mask)         : 0.6074\n",
      "mAP50-95(Mask)      : 0.2786\n",
      "Box_Loss(Val)       : 2.2198\n",
      "Seg_Loss(Val)       : 4.9191\n",
      "Cls_Loss(Val)       : 1.7298\n",
      "Fitness             : 0.5922\n",
      "Speed_Pre           : 6.8076\n",
      "Speed_Inf           : 2238.3041\n",
      "Speed_Post          : 3.8371\n",
      "F1-Score(Mask)      : 0.6083\n",
      "Dice_Coefficient    : 0.6083\n",
      "Mean_IoU            : 0.6074\n",
      "Accuracy            : 0.6186\n",
      "Specificity         : 0.5385\n",
      "All 20 metrics and visualization curves saved to ./results\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model1 = YOLO(\"./kaggle/working/bone_fracture_model/weights/best.pt\")\n",
    "    print(\"i am using best model\")\n",
    "    model=model1\n",
    "except:\n",
    "    print(\"last checkpoint\")\n",
    "def calculate_and_save_detailed_metrics(model, results, results_dir):\n",
    "    print(\"Gathering comprehensive metrics...\")\n",
    "    \n",
    "    # 1. Base Metrics from training summary\n",
    "    res_df = pd.read_csv(os.path.join(WORKING_DIR, 'bone_fracture_model', 'results.csv'))\n",
    "    last_epoch = res_df.iloc[-1].to_dict()\n",
    "    \n",
    "    # 2. Validation Run for specific segmentation metrics\n",
    "    val_results = model.val(data=os.path.join(WORKING_DIR, 'data.yaml'), split='val',conf=0.2)\n",
    "    \n",
    "    # Collect Metrics (Targeting ~20)\n",
    "    final_metrics = {\n",
    "        'Precision(Box)': val_results.results_dict['metrics/precision(B)'],\n",
    "        'Recall(Box)': val_results.results_dict['metrics/recall(B)'],\n",
    "        'mAP50(Box)': val_results.results_dict['metrics/mAP50(B)'],\n",
    "        'mAP50-95(Box)': val_results.results_dict['metrics/mAP50-95(B)'],\n",
    "        'Precision(Mask)': val_results.results_dict['metrics/precision(M)'],\n",
    "        'Recall(Mask)': val_results.results_dict['metrics/recall(M)'],\n",
    "        'mAP50(Mask)': val_results.results_dict['metrics/mAP50(M)'],\n",
    "        'mAP50-95(Mask)': val_results.results_dict['metrics/mAP50-95(M)'],\n",
    "        'Box_Loss(Val)': last_epoch.get('val/box_loss', 0),\n",
    "        'Seg_Loss(Val)': last_epoch.get('val/seg_loss', 0),\n",
    "        'Cls_Loss(Val)': last_epoch.get('val/cls_loss', 0),\n",
    "        'Fitness': val_results.fitness,\n",
    "        'Speed_Pre': val_results.speed['preprocess'],\n",
    "        'Speed_Inf': val_results.speed['inference'],\n",
    "        'Speed_Post': val_results.speed['postprocess']\n",
    "    }\n",
    "    \n",
    "    # Derived Metrics\n",
    "    p_m = final_metrics['Precision(Mask)']\n",
    "    r_m = final_metrics['Recall(Mask)']\n",
    "    final_metrics['F1-Score(Mask)'] = 2 * (p_m * r_m) / (p_m + r_m) if (p_m + r_m) > 0 else 0\n",
    "    final_metrics['Dice_Coefficient'] = final_metrics['F1-Score(Mask)'] \n",
    "    final_metrics['Mean_IoU'] = final_metrics['mAP50(Mask)'] \n",
    "    final_metrics['Accuracy'] = (p_m + r_m) / 2 \n",
    "    final_metrics['Specificity'] = 1.0 - (1.0 - r_m) \n",
    "    \n",
    "    print(\"\\n--- Final Metrics Summary ---\")\n",
    "    for k, v in final_metrics.items():\n",
    "        print(f\"{k:<20}: {v:.4f}\")\n",
    "        \n",
    "    # Save metrics to results folder\n",
    "    pd.DataFrame([final_metrics]).to_csv(os.path.join(results_dir, 'comprehensive_metrics.csv'), index=False)\n",
    "    \n",
    "    # 3. Copy All YOLO Plots (Curves, Confusion Matrix, etc.)\n",
    "    plot_files = [\n",
    "        'results.png', 'confusion_matrix.png', 'confusion_matrix_normalized.png',\n",
    "        'F1_curve.png', 'P_curve.png', 'R_curve.png', 'PR_curve.png',\n",
    "        'labels.jpg', 'labels_correlogram.jpg'\n",
    "    ]\n",
    "    \n",
    "    for f in plot_files:\n",
    "        src = os.path.join(WORKING_DIR, 'bone_fracture_model', f)\n",
    "        if os.path.exists(src):\n",
    "            shutil.copy(src, results_dir)\n",
    "            \n",
    "    print(f\"All {len(final_metrics)} metrics and visualization curves saved to {results_dir}\")\n",
    "results=None\n",
    "calculate_and_save_detailed_metrics(model, results, RESULTS_DIR)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa81f950",
   "metadata": {},
   "source": [
    "-- Final Metrics Summary ---\n",
    "Precision(Box)      : 0.7279\n",
    "Recall(Box)         : 0.5604\n",
    "mAP50(Box)          : 0.5673\n",
    "mAP50-95(Box)       : 0.2587\n",
    "Precision(Mask)     : 0.6988\n",
    "Recall(Mask)        : 0.5385\n",
    "mAP50(Mask)         : 0.5339\n",
    "mAP50-95(Mask)      : 0.2260\n",
    "Box_Loss(Val)       : 2.2198\n",
    "Seg_Loss(Val)       : 4.9191\n",
    "Cls_Loss(Val)       : 1.7298\n",
    "Fitness             : 0.4847\n",
    "Speed_Pre           : 7.0544\n",
    "Speed_Inf           : 2259.7991\n",
    "Speed_Post          : 7.0608\n",
    "F1-Score(Mask)      : 0.6083\n",
    "Dice_Coefficient    : 0.6083\n",
    "Mean_IoU            : 0.5339\n",
    "Accuracy            : 0.6186\n",
    "Specificity         : 0.5385"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e71bff5",
   "metadata": {
    "papermill": {
     "duration": 0.75805,
     "end_time": "2026-01-29T13:43:19.131428",
     "exception": false,
     "start_time": "2026-01-29T13:43:18.373378",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 6. Testing & Inference visualizations\n",
    "Show final predictions on the test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c53f7d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T13:43:20.688573Z",
     "iopub.status.busy": "2026-01-29T13:43:20.688229Z",
     "iopub.status.idle": "2026-01-29T13:43:21.737598Z",
     "shell.execute_reply": "2026-01-29T13:43:21.737022Z"
    },
    "papermill": {
     "duration": 1.784009,
     "end_time": "2026-01-29T13:43:21.738973",
     "exception": false,
     "start_time": "2026-01-29T13:43:19.954964",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def run_inference_on_test():\n",
    "    test_imgs = glob(os.path.join(DATASET_DIR, 'valid', 'images', '*.jpg'))[:10]\n",
    "    os.makedirs(os.path.join(RESULTS_DIR, 'predictions'), exist_ok=True)\n",
    "    \n",
    "    for i, img_path in enumerate(test_imgs):\n",
    "        res = model.predict(img_path, conf=0.25)\n",
    "        annotated_img = res[0].plot()\n",
    "        \n",
    "        save_path = os.path.join(RESULTS_DIR, 'predictions', f'test_pred_{i}.jpg')\n",
    "        cv2.imwrite(save_path, annotated_img)\n",
    "        \n",
    "        if i < 3:\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.imshow(cv2.cvtColor(annotated_img, cv2.COLOR_BGR2RGB))\n",
    "            plt.title(f\"Test Image {i}\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "run_inference_on_test()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766fafa2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-01-29T13:43:23.325636Z",
     "iopub.status.busy": "2026-01-29T13:43:23.325321Z",
     "iopub.status.idle": "2026-01-29T13:43:30.709511Z",
     "shell.execute_reply": "2026-01-29T13:43:30.708793Z"
    },
    "papermill": {
     "duration": 8.239166,
     "end_time": "2026-01-29T13:43:30.712180",
     "exception": false,
     "start_time": "2026-01-29T13:43:22.473014",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "def run_inference_on_test(model, dataset_dir, results_dir, num_samples=10, conf_threshold=0.25):\n",
    "    \"\"\"\n",
    "    Run inference on test images and display ONLY segmentation masks (no boxes)\n",
    "    \n",
    "    Args:\n",
    "        model: Trained YOLO segmentation model\n",
    "        dataset_dir: Path to dataset directory\n",
    "        results_dir: Path to save results\n",
    "        num_samples: Number of test images to process\n",
    "        conf_threshold: Confidence threshold for predictions\n",
    "    \"\"\"\n",
    "    test_imgs = glob(os.path.join(dataset_dir, 'valid', 'images', '*.jpg'))[:num_samples]\n",
    "    pred_dir = os.path.join(results_dir, 'predictions')\n",
    "    os.makedirs(pred_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Running inference on {len(test_imgs)} test images...\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    for i, img_path in enumerate(test_imgs):\n",
    "        # Run prediction\n",
    "        results = model.predict(img_path, conf=conf_threshold, verbose=False)\n",
    "        \n",
    "        # Get original image\n",
    "        original_img = cv2.imread(img_path)\n",
    "        img_rgb = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Create mask overlay image\n",
    "        mask_overlay = create_mask_only_visualization(original_img, results[0])\n",
    "        \n",
    "        # Save the mask overlay\n",
    "        save_path = os.path.join(pred_dir, f'test_pred_{i}.jpg')\n",
    "        cv2.imwrite(save_path, mask_overlay)\n",
    "        \n",
    "        # Display first 3 images\n",
    "        if i < 3:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "            \n",
    "            # Original image\n",
    "            axes[0].imshow(img_rgb)\n",
    "            axes[0].set_title(f'Original Image {i}', fontsize=14)\n",
    "            axes[0].axis('off')\n",
    "            \n",
    "            # Mask overlay\n",
    "            axes[1].imshow(cv2.cvtColor(mask_overlay, cv2.COLOR_BGR2RGB))\n",
    "            axes[1].set_title(f'Segmentation Mask {i}', fontsize=14)\n",
    "            axes[1].axis('off')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        print(f\"  Processed {i+1}/{len(test_imgs)}: {os.path.basename(img_path)}\")\n",
    "    \n",
    "    print(f\"\\nâœ… All predictions saved to: {pred_dir}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "\n",
    "def create_mask_only_visualization(img, result, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Create visualization with ONLY segmentation masks (no bounding boxes)\n",
    "    \n",
    "    Args:\n",
    "        img: Original image (BGR)\n",
    "        result: YOLO prediction result object\n",
    "        alpha: Transparency of mask overlay (0-1)\n",
    "    \n",
    "    Returns:\n",
    "        Image with mask overlay (BGR)\n",
    "    \"\"\"\n",
    "    # Create a copy of the original image\n",
    "    overlay = img.copy()\n",
    "    \n",
    "    # Check if there are any masks\n",
    "    if result.masks is None or len(result.masks) == 0:\n",
    "        # No detections - return original image with text\n",
    "        cv2.putText(overlay, 'No fractures detected', (10, 30),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "        return overlay\n",
    "    \n",
    "    # Get masks data\n",
    "    masks = result.masks.data.cpu().numpy()  # Get mask arrays\n",
    "    \n",
    "    # Color palette for different instances (if multiple fractures detected)\n",
    "    colors = [\n",
    "        (255, 0, 0),      # Red\n",
    "        (0, 255, 0),      # Green\n",
    "        (0, 0, 255),      # Blue\n",
    "        (255, 255, 0),    # Cyan\n",
    "        (255, 0, 255),    # Magenta\n",
    "        (0, 255, 255),    # Yellow\n",
    "        (128, 0, 255),    # Purple\n",
    "        (255, 128, 0),    # Orange\n",
    "    ]\n",
    "    \n",
    "    # Process each detected mask\n",
    "    for idx, mask in enumerate(masks):\n",
    "        # Resize mask to image size\n",
    "        mask_resized = cv2.resize(mask, (img.shape[1], img.shape[0]))\n",
    "        \n",
    "        # Convert to binary mask\n",
    "        binary_mask = (mask_resized > 0.5).astype(np.uint8)\n",
    "        \n",
    "        # Choose color for this instance\n",
    "        color = colors[idx % len(colors)]\n",
    "        \n",
    "        # Create colored mask\n",
    "        colored_mask = np.zeros_like(img)\n",
    "        colored_mask[binary_mask == 1] = color\n",
    "        \n",
    "        # Blend with original image\n",
    "        overlay = cv2.addWeighted(overlay, 1, colored_mask, alpha, 0)\n",
    "        \n",
    "        # Optional: Draw contour around mask for better visibility\n",
    "        contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cv2.drawContours(overlay, contours, -1, color, 2)\n",
    "    \n",
    "    # Add detection count text\n",
    "    num_detections = len(masks)\n",
    "    text = f'Detected: {num_detections} fracture{\"s\" if num_detections > 1 else \"\"}'\n",
    "    cv2.putText(overlay, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    \n",
    "    return overlay\n",
    "\n",
    "\n",
    "def create_side_by_side_comparison(model, dataset_dir, results_dir, num_samples=5, conf_threshold=0.25):\n",
    "    \"\"\"\n",
    "    Create side-by-side comparison: Original | Ground Truth | Prediction\n",
    "    \"\"\"\n",
    "    test_imgs = glob(os.path.join(dataset_dir, 'valid', 'images', '*.jpg'))[:num_samples]\n",
    "    comparison_dir = os.path.join(results_dir, 'comparisons')\n",
    "    os.makedirs(comparison_dir, exist_ok=True)\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Creating comparison visualizations...\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    for i, img_path in enumerate(test_imgs):\n",
    "        # Load original image\n",
    "        img = cv2.imread(img_path)\n",
    "        img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Load ground truth mask\n",
    "        base_name = os.path.basename(img_path).replace('.jpg', '.txt')\n",
    "        label_path = os.path.join(dataset_dir, 'valid', 'labels', base_name)\n",
    "        gt_mask = create_ground_truth_mask(img, label_path)\n",
    "        \n",
    "        # Get prediction\n",
    "        results = model.predict(img_path, conf=conf_threshold, verbose=False)\n",
    "        pred_mask = create_mask_only_visualization(img, results[0], alpha=0.6)\n",
    "        \n",
    "        # Create comparison figure\n",
    "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "        \n",
    "        axes[0].imshow(img_rgb)\n",
    "        axes[0].set_title('Original Image', fontsize=12, fontweight='bold')\n",
    "        axes[0].axis('off')\n",
    "        \n",
    "        axes[1].imshow(cv2.cvtColor(gt_mask, cv2.COLOR_BGR2RGB))\n",
    "        axes[1].set_title('Ground Truth', fontsize=12, fontweight='bold')\n",
    "        axes[1].axis('off')\n",
    "        \n",
    "        axes[2].imshow(cv2.cvtColor(pred_mask, cv2.COLOR_BGR2RGB))\n",
    "        axes[2].set_title('Prediction', fontsize=12, fontweight='bold')\n",
    "        axes[2].axis('off')\n",
    "        \n",
    "        plt.suptitle(f'Comparison {i+1}: {os.path.basename(img_path)}', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        # Save comparison\n",
    "        save_path = os.path.join(comparison_dir, f'comparison_{i}.png')\n",
    "        plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"  Created comparison {i+1}/{len(test_imgs)}\")\n",
    "    \n",
    "    print(f\"\\nâœ… Comparisons saved to: {comparison_dir}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "\n",
    "def create_ground_truth_mask(img, label_path):\n",
    "    \"\"\"\n",
    "    Create visualization of ground truth segmentation mask from YOLO format\n",
    "    \"\"\"\n",
    "    overlay = img.copy()\n",
    "    \n",
    "    if not os.path.exists(label_path):\n",
    "        cv2.putText(overlay, 'No ground truth', (10, 30),\n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)\n",
    "        return overlay\n",
    "    \n",
    "    h, w = img.shape[:2]\n",
    "    \n",
    "    # Read YOLO segmentation labels\n",
    "    with open(label_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    \n",
    "    color = (0, 255, 0)  # Green for ground truth\n",
    "    \n",
    "    for line in lines:\n",
    "        parts = line.strip().split()\n",
    "        if len(parts) < 7:  # Need at least class + 3 points\n",
    "            continue\n",
    "        \n",
    "        # Parse polygon coordinates (skip class_id)\n",
    "        coords = list(map(float, parts[1:]))\n",
    "        \n",
    "        # Convert normalized coords to pixel coords\n",
    "        points = []\n",
    "        for i in range(0, len(coords), 2):\n",
    "            x = int(coords[i] * w)\n",
    "            y = int(coords[i+1] * h)\n",
    "            points.append([x, y])\n",
    "        \n",
    "        points = np.array(points, dtype=np.int32)\n",
    "        \n",
    "        # Create mask\n",
    "        mask = np.zeros((h, w), dtype=np.uint8)\n",
    "        cv2.fillPoly(mask, [points], 1)\n",
    "        \n",
    "        # Create colored overlay\n",
    "        colored_mask = np.zeros_like(img)\n",
    "        colored_mask[mask == 1] = color\n",
    "        \n",
    "        # Blend\n",
    "        overlay = cv2.addWeighted(overlay, 1, colored_mask, 0.5, 0)\n",
    "        \n",
    "        # Draw contour\n",
    "        contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        cv2.drawContours(overlay, contours, -1, color, 2)\n",
    "    \n",
    "    return overlay\n",
    "\n",
    "\n",
    "def run_inference_masks_only(model, dataset_dir, results_dir, num_samples=10, show_plots=True):\n",
    "    \"\"\"\n",
    "    Simple function - just masks, no boxes\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ” Running Inference - Masks Only (No Boxes)\")\n",
    "    \n",
    "    test_imgs = glob(os.path.join(dataset_dir, 'valid', 'images', '*.jpg'))[:num_samples]\n",
    "    pred_dir = os.path.join(results_dir, 'predictions_masks_only')\n",
    "    os.makedirs(pred_dir, exist_ok=True)\n",
    "    \n",
    "    for i, img_path in enumerate(test_imgs):\n",
    "        results = model.predict(img_path, conf=0.25, verbose=False)\n",
    "        \n",
    "        # Original image\n",
    "        img = cv2.imread(img_path)\n",
    "        \n",
    "        # Create mask-only visualization\n",
    "        mask_img = create_mask_only_visualization(img, results[0], alpha=0.6)\n",
    "        \n",
    "        # Save\n",
    "        save_path = os.path.join(pred_dir, f'mask_only_{i}.jpg')\n",
    "        cv2.imwrite(save_path, mask_img)\n",
    "        \n",
    "        # Display first few\n",
    "        if show_plots and i < 3:\n",
    "            plt.figure(figsize=(10, 10))\n",
    "            plt.imshow(cv2.cvtColor(mask_img, cv2.COLOR_BGR2RGB))\n",
    "            plt.title(f'Segmentation Mask - Test Image {i}', fontsize=14, fontweight='bold')\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "    \n",
    "    print(f\"âœ… Saved {len(test_imgs)} mask visualizations to: {pred_dir}\\n\")\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# USAGE EXAMPLES\n",
    "# ============================================================================\n",
    "\n",
    "\"\"\"\n",
    "Example usage in your notebook:\n",
    "\"\"\"\n",
    "\n",
    "# Option 1: Simple masks only (RECOMMENDED)\n",
    "#run_inference_masks_only(model, DATASET_DIR, RESULTS_DIR, num_samples=10)\n",
    "\n",
    "# Option 2: Original function with masks only\n",
    "# run_inference_on_test(model, DATASET_DIR, RESULTS_DIR, num_samples=10)\n",
    "\n",
    "# Option 3: Side-by-side comparison with ground truth\n",
    "create_side_by_side_comparison(model, DATASET_DIR, RESULTS_DIR, num_samples=10)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 3860653,
     "sourceId": 6696884,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31260,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 27629.269485,
   "end_time": "2026-01-29T13:43:34.489265",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2026-01-29T06:03:05.219780",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
